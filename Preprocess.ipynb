{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c89237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282bc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径列表\n",
    "file_paths = [\n",
    "    'D:\\\\学习\\\\Postgra\\\\Text Technologies for Data Science\\\\Project\\\\Query-Processing\\\\OneDrive_1_2024-2-16\\\\combined123\\\\combined_part_1.jsonl',\n",
    "    'D:\\\\学习\\\\Postgra\\\\Text Technologies for Data Science\\\\Project\\\\Query-Processing\\\\OneDrive_1_2024-2-16\\\\combined123\\\\combined_part_2.jsonl',\n",
    "    'D:\\\\学习\\\\Postgra\\\\Text Technologies for Data Science\\\\Project\\\\Query-Processing\\\\OneDrive_1_2024-2-16\\\\combined123\\\\combined_part_3.jsonl',\n",
    "    'D:\\\\学习\\\\Postgra\\\\Text Technologies for Data Science\\\\Project\\\\Query-Processing\\\\OneDrive_1_2024-2-16\\\\combined123\\\\combined_part_4.jsonl',\n",
    "    'D:\\\\学习\\\\Postgra\\\\Text Technologies for Data Science\\\\Project\\\\Query-Processing\\\\OneDrive_1_2024-2-16\\\\combined123\\\\combined_part_5.jsonl'\n",
    "]\n",
    "\n",
    "# 打开一个新文件用于写入所有标题\n",
    "with open('all_titles.txt', 'w', encoding='utf-8') as out_file:\n",
    "    # 遍历文件路径\n",
    "    for file_path in file_paths:\n",
    "        # 打开文件并逐行读取\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # 将每行的内容从JSON字符串转换为字典\n",
    "                article = json.loads(line)\n",
    "                # 替换掉标题中的换行符'\\n'，然后写入到输出文件中\n",
    "                title = article['title'].replace('\\n', ' ')\n",
    "                out_file.write(title + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff34bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    预处理文本：转换为小写并去除非字母数字字符\n",
    "    \"\"\"\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 使用正则表达式去除非字母数字字符\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# 输入文件路径\n",
    "input_file_path = 'all_titles.txt'\n",
    "# 输出文件路径\n",
    "output_file_path = 'preprocessed_titles.txt'\n",
    "\n",
    "# 读取输入文件，预处理每一行，然后写入到输出文件\n",
    "with open(input_file_path, 'r', encoding='utf-8') as in_file, \\\n",
    "     open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    for line in in_file:\n",
    "        preprocessed_line = preprocess_text(line)\n",
    "        out_file.write(preprocessed_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 45, 100)           900800    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 45, 150)           150600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9008)              909808    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,061,608\n",
      "Trainable params: 2,061,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Toobox\\Anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - ETA: 0s - loss: 9.3804 - accuracy: 0.0610\n",
      "Epoch 1: loss improved from inf to 9.38040, saving model to model_checkpoint2.h5\n",
      "393/393 [==============================] - 36s 82ms/step - loss: 9.3804 - accuracy: 0.0610\n",
      "Epoch 2/100\n",
      "393/393 [==============================] - ETA: 0s - loss: 8.8318 - accuracy: 0.0626\n",
      "Epoch 2: loss improved from 9.38040 to 8.83185, saving model to model_checkpoint2.h5\n",
      "393/393 [==============================] - 32s 81ms/step - loss: 8.8318 - accuracy: 0.0626\n",
      "Epoch 3/100\n",
      " 60/393 [===>..........................] - ETA: 26s - loss: 8.6700 - accuracy: 0.0729"
     ]
    }
   ],
   "source": [
    "file_path = 'preprocessed_titles.txt'\n",
    "\n",
    "titles = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i < 5000:  # 仅读取前5000行\n",
    "            title = line.strip().lower()\n",
    "            titles.append(title)\n",
    "        else:\n",
    "            break  # 当读取到5000行时，退出循环\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(titles)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in titles:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 填充序列以保持统一长度\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# 分割为特征和标签\n",
    "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "label = to_categorical(label, num_classes=total_words)\n",
    "\n",
    "# 定义模型\n",
    "def create_model(total_words, max_sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    # 编译模型\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "model = create_model(total_words, max_sequence_len)\n",
    "model.summary()\n",
    "\n",
    "# 设置checkpoint\n",
    "checkpoint_path = 'model_checkpoint2.h5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# 训练模型\n",
    "model.fit(predictors, label, epochs=100, verbose=1, callbacks=[checkpoint],batch_size=64)\n",
    "\n",
    "# 保存tokenizer\n",
    "dump(tokenizer, open('tokenizer2.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70dab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型：\n",
    "# 加载模型\n",
    "model = load_model('model_checkpoint.h5')\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predictions = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predictions, axis=-1)[0]\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"Deep learning\"\n",
    "next_words = 10  # 生成标题的长度\n",
    "\n",
    "generated_title = generate_text(seed_text, next_words, model, tokenizer, max_sequence_len)\n",
    "print(generated_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
